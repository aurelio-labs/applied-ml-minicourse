{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity\n",
    "\n",
    "A key technology or *\"idea\"* required to power our app is *semantic similarity*. That is comparing items based on their human meaning.\n",
    "\n",
    "We'll start by applying this concept to text, as it is one of the more intuitive modalities for this idea.\n",
    "\n",
    "A traditional search/comparison between text will look at words or sub-words and compare the frequency of important words across the items being compared. For example:\n",
    "\n",
    "üè¶ \"The **Bank** of England\"\n",
    "\n",
    "üåæ \"A grassy **bank**\"\n",
    "\n",
    "üõ© \"A plane **bank**s\"\n",
    "\n",
    "The word **\"bank\"** being shared by each phrase means a traditional keyword comparison may view these phrases as similar. However, from a human *semantic* perspective they are not similar at all. Each has a completely different meaning.\n",
    "\n",
    "A *semantic similarity* technology would be able to recognize this and understand the difference between each phrase based on the surrounding words (the context).\n",
    "\n",
    "## Embedding Models\n",
    "\n",
    "To find \"semantic similarity\" we need models that have been trained to understand patterns in language. A popular example of this is BERT. BERT is a well known transformer model that has been trained on huge amounts of text data.\n",
    "\n",
    "Thanks to the huge training dataset used by BERT and other language models, they are able grasp linguistic patterns surprisingly well.\n",
    "\n",
    "From here we can further fine-tune (i.e. train) the language model on pairs of similar and dissimilar text. We call this *contrastive learning*.\n",
    "\n",
    "A model fine-tuned with contrastive learning methods is able to transform sentences into *vector embeddings*.\n",
    "\n",
    "<img src=\"https://github.com/jamescalam/applied-ml-minicourse/raw/main/images/encoder-vector-space.png\" style=\"width:70%\">\n",
    "\n",
    "During the contrastive learning process, the model learns to place similar sentences in a similar vector space and dissimilar sentences in a dissimilar vector space.\n",
    "\n",
    "<img src=\"https://github.com/jamescalam/applied-ml-minicourse/raw/main/images/vector-space.png\" style=\"width:70%\">\n",
    "\n",
    "Using the logic, we can transform sentences into *meaningful* vectors and compare them with *similarity metrics*. These metrics are simply calculations that compare the *similarity or distance* between vectors.\n",
    "\n",
    "<div style=\"display:flex\">\n",
    "  <div style=\"flex:50%;padding:5px\">\n",
    "    <img src=\"https://github.com/jamescalam/applied-ml-minicourse/raw/main/images/vec-search-distance.png\" alt=\"Euclidean distance\" style=\"width:100%\">\n",
    "  </div>\n",
    "  <div style=\"flex:50%;padding:5px\">\n",
    "    <img src=\"https://github.com/jamescalam/applied-ml-minicourse/raw/main/images/vec-search-cosine.png\" alt=\"Cosine similarity\" style=\"width:100%\">\n",
    "  </div>\n",
    "</div>\n",
    "\n",
    "The `sentence-transformers` library is a popular way of creating vector embeddings from text, let's take a look at how to use it.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "We start by installing the `sentence-transformers` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize an existing sentence transformer model called [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can encode text into vectors like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = [\n",
    "    \"the bank of england\",  # phrase A\n",
    "    \"a british financial institution\",  # phrase B\n",
    "    \"a grassy bank\"  # phrase C\n",
    "]\n",
    "\n",
    "embeddings = model.encode(phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compare them using cosine similarity. Again, `sentence-transformers` provides utilities for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    0.73: \"the bank of england\" vs \"a british financial institution\"\n",
      "    0.54: \"the bank of england\" vs \"a grassy bank\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "ab = cos_sim(embeddings[0], embeddings[1]).item()\n",
    "ac = cos_sim(embeddings[0], embeddings[2]).item()\n",
    "\n",
    "print(f\"\"\"\n",
    "    {round(ab, 2)}: \"{phrases[0]}\" vs \"{phrases[1]}\"\n",
    "    {round(ac, 2)}: \"{phrases[0]}\" vs \"{phrases[2]}\"\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity score between phrase *A* and *B* is correctly much greater than that between *A* and *C* despite sharing no words. This is because we are looking at *semantic similarity* and not the traditional method of keyword overlap."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
