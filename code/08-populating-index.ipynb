{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jamescalam/applied-ml-minicourse/blob/main/code/08-populating-index.ipynb)\n",
    "\n",
    "# 08: Populating the Index\n",
    "\n",
    "In this chapter we will use everything we have learned so far about the diffusion pipeline, vector search, and cloud storage to build an initial database of *prompt vectors* and their respective images.\n",
    "\n",
    "<img src=\"https://github.com/jamescalam/applied-ml-minicourse/raw/main/images/generating-images.png\" style=\"width:80%\">\n",
    "\n",
    "To do all of this we will start by initializing four components:\n",
    "\n",
    "1. A prompt dataset\n",
    "2. The `StableDiffusionPipeline`\n",
    "3. Cloud Storage\n",
    "4. Pinecone\n",
    "\n",
    "## Prompt Dataset\n",
    "\n",
    "To begin we will download the prompt dataset from Hugging Face *Datasets* called `'bartman081523/stable-diffusion-discord-prompts'`. It contains almost *3.9M* prompts. We will not index them all in this example, but feel free to do so if you're feeling *very* patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration bartman081523--stable-diffusion-discord-prompts-c1485b9878be2896\n",
      "Reusing dataset text (/Users/jamesbriggs/.cache/huggingface/datasets/bartman081523___text/bartman081523--stable-diffusion-discord-prompts-c1485b9878be2896/0.0.0/4b86d314f7236db91f0a0f5cda32d4375445e64c5eda2692655dd99c2dac68e8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 3884798\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "prompts = load_dataset(\n",
    "    'bartman081523/stable-diffusion-discord-prompts',\n",
    "    split='train'\n",
    ")\n",
    "prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StableDiffusionPipeline\n",
    "\n",
    "We will be relying on the stable diffusion pipeline for both prompt vector creation *and* generating the images. It should be moved to GPU where possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# set the hardware device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# init all of the pipeline models and move them to a given GPU\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\",\n",
    "  \tuse_auth_token=\"<<ACCESS_TOKEN>>\"\n",
    ")\n",
    "pipe.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Cloud Storage\n",
    "\n",
    "Next we need to connect to our *Cloud Storage* instance. As before we do this using the `cloud-storage.json` credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# set credentials\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'cloud-storage.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And connect to the `diffusion` bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "# connect to bucket (we named it 'diffusion')\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.get_bucket('diffusion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Pinecone\n",
    "\n",
    "We must also initialize our connection to Pinecone and create a vector index to store the *prompt vectors*. First we initialize a connection with our [API key](https://app.pinecone.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key='<<YOUR_API_KEY>>',  # app.pinecone.io\n",
    "    environment='us-west1-gcp'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a new index with the CLIP pooled embedding dimensionality and using cosine similarity metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = pipe.text_encoder.get_word_embedding_dimension()\n",
    "print(dim)\n",
    "\n",
    "index_name = 'diffusion'\n",
    "\n",
    "# create index\n",
    "pinecone.create_index(\n",
    "    name=index_name,\n",
    "    dimension=dim,\n",
    "    metric='cosine'\n",
    ")\n",
    "\n",
    "# connect to index\n",
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should see that the index is currently empty as we have not added anything yet. Let's take a look at adding a single item before we move on to doing this for a larger number of samples.\n",
    "\n",
    "## Adding a Record\n",
    "\n",
    "When adding a record we will perform a few steps:\n",
    "\n",
    "1. Generate *prompt vector*\n",
    "2. Generate image\n",
    "3. Create unique ID shared between the prompt vector and image\n",
    "4. Upload image to Cloud Storage\n",
    "5. Insert prompt vector to Pinecone\n",
    "`\n",
    "These five steps will be repeating for every prompt. Step **2** is very time consuming and hence why we will not do this for the full 3.9M+ records.\n",
    "\n",
    "### Generating a Prompt Vector\n",
    "\n",
    "Starting with the prompt vector, we generate this using the first two components of our `StableDiffusionPipeline`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a person surfing\"\n",
    "\n",
    "# encode prompt to mean_pooled vector\n",
    "tokens = pipe.tokenizer(\n",
    "    prompt, padding='max_length',\n",
    "    return_tensors='pt'\n",
    ").to(device)\n",
    "vec = pipe.text_encoder(**tokens)['mean_pooled'].detach().cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Image\n",
    "\n",
    "To generate the equivalent image for our prompt vector, we simply run the prompt through the full pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.10.7 64-bit' requires ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "out = pipe(prompt)\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And view the image like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique ID\n",
    "\n",
    "The unique ID will be shared by both the prompt vector that will be stored in Pinecone, and the image to be stored in Cloud Storage. We use the `uuid` library to create it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c5280325-206f-44d8-a9a4-0b8cdf9fd2cd'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "_id = str(uuid.uuid4())\n",
    "_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload to Cloud Storage\n",
    "\n",
    "Next we use the unique ID to upload the generated image to GCP Cloud Storage. First we save the image to file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.images[0].save('tmp.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then upload using the unique ID as a filename:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.blob(f'{_id}.png')\n",
    "blob.upload_from_filename('tmp.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert to Pinecone\n",
    "\n",
    "The final step is inserting the prompt vector and any relevant metadata in Pinecone. Every record in Pinecone requires an ID, vector, and *optionally* metadata dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create metadata\n",
    "metadata = {\n",
    "    \"prompt\": prompt\n",
    "}\n",
    "# format as tuple\n",
    "to_upsert = (_id, vec, metadata)\n",
    "# upsert to index\n",
    "index.upsert([to_upsert])\n",
    "\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now see that a single item exists within the index.\n",
    "\n",
    "All we need to do now is repeat this process for many records in our prompts dataset.\n",
    "\n",
    "## Building our Database\n",
    "\n",
    "We will repeat the above steps and batch prompts together where possible.\n",
    "\n",
    "Before starting, we will trim down the prompts dataset. Many smaller prompts are nonscensical or not very interesting, so we will filter those out first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f468a12a4665427da4a22c983835f99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3885 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 3582032\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = prompts.filter(\n",
    "    lambda x: len(x['text']) > 30\n",
    ")\n",
    "prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remove duplicates..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3546559"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompts = list(set(prompts['text']))\n",
    "len(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a reasonable database of images with just 10-20K of these prompts. Naturally the more the merrier, but to avoid finishing this minicourse next year let's go with a smaller number of `10000` (go for fewer if you'd rather not wait)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = prompts[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create a function to create the prompt vectors and images called `embed_and_diffuse`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_and_diffuse(prompts: list):\n",
    "    # __diffuse images__\n",
    "    out = pipe(prompts)\n",
    "    if any(out.nsfw_content_detected):\n",
    "        return {}\n",
    "    # __create text embeddings__\n",
    "    inputs = text_inputs = pipe.tokenizer(\n",
    "        prompts, padding=True,\n",
    "        truncation=True, return_tensors='pt'\n",
    "    ).to(device)\n",
    "    text_embeds = pipe.text_encoder(**text_inputs)\n",
    "    # get pooled embeddings, move to CPU and convert to list for pinecone\n",
    "    text_embeds = text_embeds.pooler_output.cpu().tolist()\n",
    "    return {\n",
    "        'text_embeds': text_embeds,\n",
    "        'images': out.images\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now iterate through and populate the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 8  # we will run through in batches\n",
    "\n",
    "for i in tqdm(range(0, len(prompts), batch_size)):\n",
    "    i_end = min(len(prompts), i+batch_size)\n",
    "    # get batch of prompts\n",
    "    prompts = prompts[i:i_end]\n",
    "    data = embed_and_diffuse(prompts)\n",
    "    if not data:\n",
    "        # nsfw content detected so skip\n",
    "        continue\n",
    "    # create batch of ids\n",
    "    ids = [str(uuid.uuid4()) for _ in range(len(prompts))]\n",
    "    meta = []\n",
    "    # add images to cloud storage\n",
    "    for _id, image, prompt in zip(ids, data['images'], prompts):\n",
    "        image.save('tmp.png', format='png')\n",
    "        # push to cloud storage\n",
    "        blob = bucket.blob(f'images/{_id}.png')\n",
    "        blob.upload_from_filename('tmp.png')\n",
    "    # create metadata\n",
    "    meta = [{\n",
    "            'prompt': prompt,\n",
    "            'image_url': f'images/{_id}.png'\n",
    "        } for _id, prompt in zip(ids, prompts)]\n",
    "    # add to pinecone\n",
    "    index.upsert(zip(ids, data['text_embeds'], meta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this has completed running we should find that we have 10,001 images uploaded in our index (+1 for the first `\"a person surfing\"` prompt).\n",
    "\n",
    "If the above is taking too long to run, feel free to stop the execution and continue with the course. 10K images is not a prerequisite for the remainder of the course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
