{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/jamescalam/applied-ml-minicourse/blob/main/code/05-diffusion-search.ipynb)\n",
    "\n",
    "# 05: Applying Semantic Search to Diffusion\n",
    "\n",
    "Semantic search is prominent in text search but can be applied to any modality that we can think of as containing some degree of potential human meaning. We could search across [text and images based on their *meaningful* content](https://www.pinecone.io/learn/clip/), perform [question-answering across video](https://www.pinecone.io/learn/openai-whisper/), and more.\n",
    "\n",
    "The question now is, how do we apply search to make diffusion more efficient?\n",
    "\n",
    "Ideally, we want each diffusion image to be represented by a vector. When a user requests a new diffusion image, we convert their query into a vector and cross-check it against past diffusion vectors. If we find that something similar has already been generated we can return that rather than waiting through the lengthy diffusion process.\n",
    "\n",
    "## Finding Diffusion Vectors\n",
    "\n",
    "Let's start by initializing a stable diffusion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "\n",
    "# set the hardware device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# init all of the pipeline models and move them to a given GPU\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\",\n",
    "  \tuse_auth_token=\"<<ACCESS_TOKEN>>\"\n",
    ")\n",
    "pipe.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the diffusion pipeline, it isn't immediately clear where we can extract meaningful vectors from:\n",
    "\n",
    "![Stable Diffusion pipeline](https://github.com/jamescalam/applied-ml-minicourse/raw/main/images/stable-diffusion-pipeline-annotated.png)\n",
    "\n",
    "There is a model called *CLIP* that acts as one of the earliest steps in the pipeline. This model is actually fine-tuned with a contrastive loss function between *(text, image)* pairs.\n",
    "\n",
    "That means CLIP should, in theory, be able to produce the meaningful vectors we need. However, we can see that CLIP *isn't* outputting a single vector embedding. CLIP is outputting *77* vectors, a single vector for each token fed into the model.\n",
    "\n",
    "We can see this *77x768* dimension output when processing a prompt through the first two steps of the pipeline (the `tokenizer` and CLIP `text_encoder`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"a person surfing\"\n",
    "\n",
    "# tokenize the prompt\n",
    "prompt_inputs = pipe.tokenizer(\n",
    "    prompt, return_tensors='pt',\n",
    "    padding='max_length'\n",
    ").to(device)\n",
    "# create prompt encoding\n",
    "out = pipe.text_encoder(**prompt_inputs)\n",
    "# extract CLIP embedding\n",
    "prompt_embeds = out['last_hidden_state']\n",
    "prompt_embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To transform these 77-vectors into a single vector we can perform a *mean pooling* operation.\n",
    "\n",
    "<img src=\"https://github.com/jamescalam/applied-ml-minicourse/raw/main/images/mean-pooling.png\" style=\"width:60%\">\n",
    "\n",
    "Applying mean pooling means we average the values across each of the 768-dimensions, producing a single 768-d vector.\n",
    "\n",
    "<img src=\"https://github.com/jamescalam/applied-ml-minicourse/raw/main/images/vector-extraction.png\" style=\"width:80%\">\n",
    "\n",
    "Fortunately, CLIP does in fact produce this mean-pooled vector by default despite not being used by the diffusion pipeline. It is output within a `'pooler_output'` tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract *pooled* CLIP embedding\n",
    "prompt_embeds = out['pooler_output']\n",
    "prompt_embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because these pooled *\"prompt vectors\"* are created before the long diffusion process begins, we can build them quickly and use them to retrieve any past, similar generations.\n",
    "\n",
    "<img src=\"https://github.com/jamescalam/applied-ml-minicourse/raw/main/images/creating-embeddings.png\" style=\"width:60%\">\n",
    "\n",
    "These *prompt vectors* are one side of what we will need to store from each diffusion generation. The other is the images themselves. For that we will need to use another service that we will explore in the next chapters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
