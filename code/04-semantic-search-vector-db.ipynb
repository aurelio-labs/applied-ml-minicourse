{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Search with Vector DBs\n",
    "\n",
    "We now understand the essentials of semantic similarity, but how do we scale something like this to millions or even billions of records?\n",
    "\n",
    "For this we need a *vector database*. A vector database is a framework around something we call a *vector index*. The vector index is a data structure containing vectors like those we created in the previous chapter.\n",
    "\n",
    "## Approximate Search\n",
    "\n",
    "For the search through a vector index to be scalable it must use an *approximate nearest neighbors* structure. This structure can exist in many forms, but they all boil down to *approximating* the most similar vectors.\n",
    "\n",
    "Why do we need to approximate the answer? Well, if we want to search through just one million vectors, using an *exact* search we must perform *one million* comparisons. At 1M items, this is just about doable, but as soon as we scale further it becomes very slow.\n",
    "\n",
    "Therefore, we approximate the search space. Meaning we can scale to much larger indexes with ease.\n",
    "\n",
    "<img src=\"https://github.com/jamescalam/applied-ml-minicourse/raw/main/images/vec-db-scale.png\" style=\"width:70%\">\n",
    "\n",
    "Once we have an approximate vector index, there's still more needed to create a vector database.\n",
    "\n",
    "## Vector Databases\n",
    "\n",
    "Using a good vector database we should see data management capabilities like record insertion, deletion, updates, etc. We should find the ability to add *metadata* to vectors and filter the search space based on metadata. Ideally, we wouldn't want to manage all of this ourselves, and fortunately there are services that do this for us. We will be using the Pinecone vector database for this.\n",
    "\n",
    "Using Pinecone we are able to store and search through upto 5M vectors on their free tier, more than enough for most use-cases (including ours).\n",
    "\n",
    "## Implementation\n",
    "\n",
    "For this chapter we will download a question-answering dataset, encode it with a `sentence-transformers` model, and index and search with the Pinecone vector database. Let's begin by installing the prerequsites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq datasets pinecone-client sentence-transformers tqdm --extra-index-url https://download.pytorch.org/whl/cu113 torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Before creating the vector database we need a dataset that we will encode and index. We will use the ***S**tanford **Qu**estion **A**nswering **D**ataset* via another Hugging Face library called `datasets`.\n",
    "\n",
    "We download the dataset like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /Users/jamesbriggs/.cache/huggingface/modules/datasets_modules/datasets/squad/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453 (last modified on Fri Sep  2 02:26:37 2022) since it couldn't be found locally at squad., or remotely on the Hugging Face Hub.\n",
      "Reusing dataset squad (/Users/jamesbriggs/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset('squad', split='train')\n",
    "squad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains many `(question, context)` pairs. Using an encoder model trained for question-answering we will be able to encode questions and their relevant contexts into a similar vector space. Due to the difference in the length of questions vs contexts this type of *semantic search* is known as ***asymmetric** semantic search*.\n",
    "\n",
    "SQuAD contains duplicate contexts as each context can answer multiple questions, so we first deduplicate our contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18891"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contexts = list(set(squad['context']))\n",
    "len(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have just ~19K contexts to index. The next step is encoding these contexts with a suitable sentence transformer.\n",
    "\n",
    "### Embedding Model\n",
    "\n",
    "As mentioned, we must use a question-answering (or QA) encoder model to build our vectors. To do this we use the [`multi-qa-MiniLM-L6-cos-v1`](https://huggingface.co/sentence-transformers/multi-qa-MiniLM-L6-cos-v1) model, we know it is suitable for QA because it has `qa` in the model name. We can also see that it is multilingual as per the `multi` in the model name, and that it should be used with cosine similarity as per the `cos` in the model name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we'll be encoding a lot of contexts we should switch the model to use a GPU device if possible. This will speed up the process significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can encode a context to create a *context vector* like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 384)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xc = model.encode([contexts[0]])\n",
    "xc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputting a *384*-dimensional vector. When encoding all of our vectors we will be continuously inserting these to our vector database, so we must now initialize that.\n",
    "\n",
    "### Vector Database\n",
    "\n",
    "We must sign up for a free API key from Pinecone at [app.pinecone.io](https://app.pinecone.io/). Once you have the API key, insert it below to initialize the connection to Pinecone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pinecone\n",
    "\n",
    "pinecone.init(\n",
    "    api_key='<<YOUR_API_KEY>>',\n",
    "    environment='us-west1-gcp'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create a single vector index using the parameters required by our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = 'squad-demo'\n",
    "\n",
    "pinecone.create_index(\n",
    "    index_name,\n",
    "    dimension=model.get_sentence_embedding_dimension(),\n",
    "    metric='cosine'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we then connect to the newly created index like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = pinecone.Index(index_name)\n",
    "\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that we're ready to begin encoding and inserting all of our contexts.\n",
    "\n",
    "### Adding the Contexts\n",
    "\n",
    "To add the contexts we will work through everything in batches of `64`. We encode a batch of 64 contexts, insert those 64 context vectors to Pinecone, and move on to the next batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "for i in tqdm(range(0, len(contexts), batch_size)):\n",
    "    # get index for end of batch\n",
    "    i_end = min(i + batch_size, len(contexts))\n",
    "    batch = contexts[i:i_end]\n",
    "    # create context vectors\n",
    "    vecs = model.encode(batch).tolist()\n",
    "    # make unique IDs for each context vector\n",
    "    ids = [f'{i}' for i in range(i, i_end)]\n",
    "    # add context text and title to metadata\n",
    "    metadata = [{\n",
    "        'context': c, 'title': squad['title'][i]\n",
    "    } for i, c in enumerate(batch)]\n",
    "    # create list of items to add to index\n",
    "    to_upsert = zip(ids, vecs, metadata)\n",
    "    index.upsert(vectors=to_upsert)\n",
    "\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now indexed all of our contexts and we can move on to *querying*.\n",
    "\n",
    "### Querying\n",
    "\n",
    "The query process is almost identical to the context encoding process. We use the QA model to encode a query, then use that to search through the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"why is Albert Einstein famous?\"\n",
    "\n",
    "xq = model.encode(query).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we query with `index.query`. We will find the top `5` most similar matches using the `top_k` parameter, and return the related metadata with `include_metadata=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/jamesbriggs/Documents/projects/applied-ml-minicourse/code/04-semantic-search-vector-db.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jamesbriggs/Documents/projects/applied-ml-minicourse/code/04-semantic-search-vector-db.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m matches \u001b[39m=\u001b[39m index\u001b[39m.\u001b[39mquery(xq, top_k\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, include_metadata\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jamesbriggs/Documents/projects/applied-ml-minicourse/code/04-semantic-search-vector-db.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m matches\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "matches = index.query(xq, top_k=5, include_metadata=True)\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8e7999f96e1b425e2d542f21b571f5a4be3e97158b0b46ea1b2500df63956ce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
